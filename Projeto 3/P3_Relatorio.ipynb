{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Heigon Soldera 217638  \n",
    "Rafael Galib 204904"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introdução\n",
    "\n",
    "Para esta atividade nos foi fornecida a ferramenta disponível em http://cs.brynmawr.edu/Courses/cs372/fall2017/Code/search.zip.\n",
    "Com ela podemos simular o jogo do Pac-Man, sendo que nos é disponível uma coleção de funcionalidades já implementadas, assim como uma coleção de layouts para as simulações.\n",
    "\n",
    "\n",
    "No pacote de arquivos, temos o motor do jogo, que conta com alguns arquivos-chave que merecem ser comentados:\n",
    "* pacman.py;\n",
    "* game.py;\n",
    "\n",
    "Ambos os arquivos utilizam de outros inerentes do pacote que não serão abordados, mas auxiliam no funcionamento do motor.\n",
    "O arquivo pacman.py é o arquivo a ser rodado por meio do comando !python pacman.py. Devido a estrutura da implementação deste arquivo, podemos passar parâmetros no comando para efetuar as simulações que precisamos.\n",
    "\n",
    "\n",
    "Os principais parâmetros são:\n",
    "\n",
    "* --layout {layout}: com este comando fazemos a seleção de qual layout específico desejamos simular;\n",
    "* --pacman {agent}: com este comando escolhemos o agente. O agente é a classe que contém a lógica de nosso pacman;\n",
    "* --numGames {nGames}: com este comando escolhemos quantas vezes queremos que um determinado agente jogue um determinado layout. O próprio motor do jogo computa as médias das pontuações, assim como o histórico de vitórias e derrotas;\n",
    "* --quetTextGraphics: este comando faz com que a parte gráfica da simulação não seja exibida na tela caso um agente tenha sido utilizado de argumento. Isso poupa muito processamento e acelera a simulação;\n",
    "\n",
    "\n",
    "    Exemplo de utilização:\n",
    "              !python pacman.py --layout smallClassic --pacman GeneticAgent --numGames 5 --quietTextGraphics\n",
    "\n",
    "Para a implementação da lógica a ser feita pelo Pac-Man, precisamos criar um arquivo terminado em 'gents.py', no caso, criamos o Agents.py. O pacote já vem com alguns arquivos 'gents.py' contendo exemplos de agentes já implementados.\n",
    "No arquivo Agents.py encontra-se a classe de agente que foi utilizada no exemplo de comando logo acima. Nesta classe encontra-se a estrutura necessária, seguindo o padrão de outros agentes de exemplo, para que o motor possa simular a nossa lógica."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# O Problema\n",
    "\n",
    "Uma vez entendido e preparado o motor do Pac-Man iremos utilizar os conceitos recém aprendidos na disciplina para abordar de duas maneiras distintas as simulações do Pac-Man: através de um algoritmo genético e de um algoritmo de aprendizado por reforço. Para esta atividade escolhemos o Q-Learning como algoritmo de aprendizado por reforço.\n",
    "\n",
    "Pede-se que os algoritmos treinem em 3 diferentes layouts: \n",
    "* smallClassic;\n",
    "* mediumClassic;\n",
    "* originalClassic;\n",
    "\n",
    "Pede-se também que se colha e compare os resultados dos dois algoritmos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evolutionary Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A seguir serão discutidos alguns pontos-chave da implementação do modelo, assim como a teoria por trás das decisões de concepções adotadas no projeto. Para a apresentação e discussão dos resultados, a parte executável do código já foi processada e sua saída será utilizada para as conclusões. O motivo disso é o custo computacional inerente do modelo evolutivo, que torna inviável que se aguarde a execução a fim de apreciar os resultados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# O modelo em si\n",
    "\n",
    "A concepção do modelo evolutivo dá-se pela concepção de que: sob um determinado ambiente, podemos fazer uma seleção de indivíduos sob um certo viés, o que resulta na disseminação de determinadas características à futuras gerações destes indivíduos.\n",
    "\n",
    "Para tanto, o ambiente deve seguir determinados critérios e a seleção deve refletir o ambiente. Assim, determinamos certos parâmetros para controlar o pacman da seguinte forma:\n",
    "\n",
    "* Primeiro parâmetro indica distância do fantasma mais próximo a qual o pacman deve fugir:\n",
    "    * Se o pacman está fugindo:\n",
    "        * Segundo parâmetro indica uma ordem de moiventos a se seguir para fugir dos fantasma (como consideramos 8 posições relativas para o fantasma teremos 8 parâmetros)\n",
    "    * Se o pacman está perseguindo comida:\n",
    "        * Terceiro parâmetro indica uma probabilidade para cada movimento legal (como consideramos 8 posições relativas para a comida teremos 8 parâmetros)\n",
    "* Um parâmetro adicional que indica quantos movimentos o pacman pode não se preocupar com fantasmas após ter comido uma cápsula.\n",
    "\n",
    "A seguir será descrito as técnicas utilizadas para o desenvolvlimento e otimização desse modelo com esses parâmetros \n",
    "\n",
    "## Population size\n",
    "\n",
    "Primeiramente, é necessária uma população de indivíduos que irão interagir com o ambiente. A população para o layout smallClassic será 200 indivíduos, mediumClassic 100 e originalClassic 50 para poupar poder computacional.\n",
    "\n",
    "## Fitness function\n",
    "\n",
    "A estes indivíduos, será utilizada uma função, tradicionalmente chamada de $fitness$, que, a partir de determinados resultados obtidos da interação dos indivíduos com o ambiente, atribuirá uma pontuação ao indivíduos, o que nos permite classificar os indivíduos sob nosso critério de seleção, ela é da seguinte forma:  \n",
    "  \n",
    "  * $fitness(indivíduo) = vitórias + comidas \\times 2 + cápsulas + min(1, tempo_de_execução) $  \n",
    "  \n",
    "## Replacement \n",
    "\n",
    "Feita a classificação, propomos que uma nova população será gerada a partir da combinação das características dos indivíduos. Tradicionalmente gera-se uma população nova com a mesma quantidade de indivíduos que a anterior, denominando-se assim como uma nova geração. Esta nova geração será composta de indivíduos que refletirão características de seus antecessores, sendo então necessária uma concepção de hereditariedade.\n",
    "\n",
    "## Selection\n",
    "\n",
    "Para que uma nova geração reflita seus antecessores, fazemos tantas seleções de indivíduos antepassados quanto forem necessárias e dessas seleções fazemos a combinação das características, e dessa combinação resulta um novo indivíduo. Em geral, combina-se dois indivíduos antepassados para gerar um novo indivíduo. A seleção de antepassados pode ser realizada de várias formas, a nível de exemplo: neste projeto selecionamos os 5 indivíduo mais bem adaptados e selecionamos 2 ao acaso, sendo que a chance de um indivíduo ser selecionado é proporcional à pontuação de sua função fitness. Repetindo essa seleção de pares quantas vezes forem necessárias, formamos uma nova geração de indivíduos. Metódo conhecido como Roulette Wheel.\n",
    "\n",
    "## CrossOver\n",
    "\n",
    "As características dos indivíduos a serem combinadas em geral são valores numérios cuja finalidade varia de implementação para implementação. No nosso modelo alguns valores simbolizam distâncias a serem consideradas, enquanto que outros valores servem como coeficientes para ações a serem tomadas em diversas situações possíveis. A esse conjunto de valores damos o nome de DNA, ou código genético, enquanto que os valores em si são denomindados alelos. A combinação dessas características também é bastante flexível. No nosso código, para cada alelo temos 50% de chance de herdar a característica de algum dos indivíduos selecionados no par. Esse método é chamado de Uniform CrossOver.\n",
    "\n",
    "## Mutation\n",
    "\n",
    "Prevendo que características menos eficazes podem disseminar-se contundentemente pelas gerações futuras, incluímos, por fim, uma chance de mutação em algum indivíduo, sendo que essa mutação consiste na alteração de algum alelo para algum valor aleatório que respeite os limites do alelo em questão. No nosso código cada novo indivíduo tem 15% de chance de passar pelo processo de mutação implementado por nós, que, para cada alelo presente no código genético do novo indivíduo, possui 10% de chance de trocar seu valor. Esse método é chamado de Uniform Mutation.\n",
    "\n",
    "## Stop Criteria\n",
    "\n",
    "O métdo para de executar somente quando encontrar um indivíduo que ganha uma quantidade mínima de jogos que varia para cada layout."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementação do Indivíduo\n",
    "\n",
    "O código dos indivíduos está escrito no arquivo Agents.py, onde lá encontra-se a classe GeneticAgent, que é o agente que possui o comportamento implementado por nós.\n",
    "\n",
    "No código temos métodos de apoio assim como métodos evocados pelo próprio motor da ferramenta fornecida a nós.\n",
    "\n",
    "O seguinte trecho de código é executado a cada iteração do jogo. Nele constitui-se o comportamento do agente.\n",
    "Primeiramente, utilizamos os métodos de apoio para calcular algumas informações como: distância ao fantasma mais próximo, distância a comida mais próxima, assim como suas direções relativas. Uma vez que exista um fantasma próximo o suficiente do pacman, o mesmo dará preferência por evitar os fantasmas, enquanto que caso não haja um fantasma suficientemente próximo, o pacman irá dar preferência a comidas.\n",
    "No código, quebramos todas as possíveis combinações de entornos que o pacman encontraria a sua volta, e, para estes, o pacman possui diferentes preferências para direções a seguir. Estas preferências são passadas por parâmetro ao pacman, constituindo seu código genético.\n",
    "\n",
    "        def getAction(self, state):\n",
    "                \"The agent receives a GameState (defined in pacman.py).\"\n",
    "                self.moves += 1\n",
    "                rw = ReadWrite()\n",
    "                posG = np.array(state.getGhostPositions())\n",
    "                posP = np.array(state.getPacmanPosition())\n",
    "                posCaps = np.array(state.getCapsules())\n",
    "                posFood = state.getFood()\n",
    "                \n",
    "                if(self.numCaps == None):\n",
    "                    self.numCaps = len(posCaps)\n",
    "                elif(self.numCaps != len(posCaps)):\n",
    "                    self.numCaps = len(posCaps)\n",
    "                    self.Attack = 0\n",
    "                elif(self.Attack >= rw.pmt[1]):\n",
    "                    self.Attack = None\n",
    "\n",
    "                if(self.Attack != None):\n",
    "                    self.Attack += 1\n",
    "\n",
    "                minDistGhost = 99999\n",
    "                indG = None\n",
    "                for i in range(len(posG)):\n",
    "                    if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                        minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "                        indG =  i\n",
    "\n",
    "                if(posG[indG][0] > posP[0]):\n",
    "                    if(posG[indG][1] > posP[1]):\n",
    "                        ghostRelativePosition = \"upright\"\n",
    "                    elif(posG[indG][1] == posP[1]):\n",
    "                        ghostRelativePosition = \"right\"\n",
    "                    else:\n",
    "                        ghostRelativePosition = \"downright\"\n",
    "\n",
    "                elif(posG[indG][0] == posP[0]):\n",
    "                    if(posG[indG][1] > posP[1]):\n",
    "                        ghostRelativePosition = \"up\"\n",
    "                    else:\n",
    "                        ghostRelativePosition = \"down\"\n",
    "                else:\n",
    "                    if(posG[indG][1] > posP[1]):\n",
    "                        ghostRelativePosition = \"upleft\"\n",
    "                    elif(posG[indG][1] == posP[1]):\n",
    "                        ghostRelativePosition = \"left\"\n",
    "                    else:\n",
    "                        ghostRelativePosition = \"downleft\"\n",
    "\n",
    "                minDistFood = 99999\n",
    "                minFood = None\n",
    "                for x in range(posFood.width):\n",
    "                    for y in range(posFood.height):\n",
    "                        if(posFood[x][y]):\n",
    "                            food = np.array([x,y])\n",
    "                            distFood = np.linalg.norm(food-posP)\n",
    "                            if(distFood<minDistFood):\n",
    "                                minDistFood = distFood\n",
    "                                minFood = food\n",
    "                \n",
    "                if(minFood[0] > posP[0]):\n",
    "                    if(minFood[1] > posP[1]):\n",
    "                        foodRelativePosition = \"upright\"\n",
    "                    elif(minFood[1] == posP[1]):\n",
    "                        foodRelativePosition = \"right\"\n",
    "                    else:\n",
    "                        foodRelativePosition = \"downright\"\n",
    "\n",
    "                elif(minFood[0] == posP[0]):\n",
    "                    if(minFood[1] > posP[1]):\n",
    "                        foodRelativePosition = \"up\"\n",
    "                    else:\n",
    "                        foodRelativePosition = \"down\"\n",
    "                else:\n",
    "                    if(minFood[1] > posP[1]):\n",
    "                        foodRelativePosition = \"upleft\"\n",
    "                    elif(minFood[1] == posP[1]):\n",
    "                        foodRelativePosition = \"left\"\n",
    "                    else:\n",
    "                        foodRelativePosition = \"downleft\"\n",
    "\n",
    "                if(len(state.getLegalPacmanActions()) == 2 ):\n",
    "                    path = \"deadend\"\n",
    "                else:\n",
    "                    path = None\n",
    "\n",
    "                if(path == \"deadend\"):\n",
    "                    if (Directions.SOUTH in state.getLegalPacmanActions()):\n",
    "                        return Directions.SOUTH\n",
    "                    elif (Directions.NORTH in state.getLegalPacmanActions()):\n",
    "                        return Directions.NORTH\n",
    "                    elif (Directions.WEST in state.getLegalPacmanActions()):\n",
    "                        return Directions.WEST\n",
    "                    else:\n",
    "                        return Directions.EAST\n",
    "\n",
    "                elif(minDistGhost < rw.pmt[0] and self.Attack == None):\n",
    "                    if(ghostRelativePosition == \"right\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[2]])\n",
    "\n",
    "                    elif(ghostRelativePosition == \"left\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[3]])\n",
    "\n",
    "                    elif(ghostRelativePosition == \"up\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[4]])\n",
    "\n",
    "                    elif(ghostRelativePosition == \"down\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[5]])\n",
    "\n",
    "                    elif(ghostRelativePosition == \"downright\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[6]])\n",
    "\n",
    "                    elif(ghostRelativePosition == \"downleft\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[7]])\n",
    "                    \n",
    "                    elif(ghostRelativePosition == \"upleft\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[8]])\n",
    "                    \n",
    "                    elif(ghostRelativePosition == \"upright\"):\n",
    "                        return self.directions(state,self.PossibleDirections[rw.pmt[9]])\n",
    "                else:\n",
    "                    if(foodRelativePosition == \"right\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[10],p2=rw.pmt[11],p3=rw.pmt[12],p4=rw.pmt[13])\n",
    "\n",
    "                    elif(foodRelativePosition == \"left\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[14],p2=rw.pmt[15],p3=rw.pmt[16],p4=rw.pmt[17])\n",
    "\n",
    "                    elif(foodRelativePosition == \"up\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[18],p2=rw.pmt[19],p3=rw.pmt[20],p4=rw.pmt[21])\n",
    "\n",
    "                    elif(foodRelativePosition == \"down\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[22],p2=rw.pmt[23],p3=rw.pmt[24],p4=rw.pmt[25])\n",
    "\n",
    "                    elif(foodRelativePosition == \"downright\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[26],p2=rw.pmt[27],p3=rw.pmt[28],p4=rw.pmt[29])\n",
    "\n",
    "                    elif(foodRelativePosition == \"downleft\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[30],p2=rw.pmt[31],p3=rw.pmt[32],p4=rw.pmt[33])\n",
    "                    \n",
    "                    elif(foodRelativePosition == \"upleft\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[34],p2=rw.pmt[35],p3=rw.pmt[36],p4=rw.pmt[37])\n",
    "                    \n",
    "                    elif(foodRelativePosition == \"upright\"):\n",
    "                        return self.directionsProb(state=state,p1=rw.pmt[38],p2=rw.pmt[39],p3=rw.pmt[40],p4=rw.pmt[41])\n",
    "\n",
    "\n",
    "Outro método importante está no arquivo ReadWrite.py, mais especificamente o construtor da classe. No construtor lemos os parâmetros que compõem o código genético do indivíduo. Perceba a manipulação de diversos arquivos .txt que foram utilizado para a comunicação entre diferentes módulos da ferramenta.\n",
    "        def __init__(self):\n",
    "\n",
    "            with open('nome.txt', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                self.nome = lines[0]\n",
    "\n",
    "            with open('agentNum_' + self.nome + '.txt') as f:\n",
    "                lines = f.readlines()\n",
    "                self.agentNum = int(lines[0])\n",
    "\n",
    "            with open('parameters_' + self.nome + '.txt') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "                self.pmt = lines[self.agentNum].split()\n",
    "\n",
    "                for i in range(len(self.pmt)):\n",
    "                    if(1<=i<=9):\n",
    "                        self.pmt[i] = int(self.pmt[i])\n",
    "                    else:\n",
    "                        self.pmt[i] = float(self.pmt[i])\n",
    "\n",
    "Com isso chegamos ao ultimo método aqui listado, pertencente à classe GeneticAgent. Temos aqui o metodo final(), que é evocado pela ferramenta no final de cada jogo, e com isso escrevendo resultados relativos a cada indivíduo da população.\n",
    "\n",
    "        def final(self, state):\n",
    "            posCaps = np.array(state.getCapsules())\n",
    "\n",
    "            with open('moves.txt', 'w') as f:\n",
    "                f.write(str(self.moves) + '\\n')\n",
    "\n",
    "            print (\"Acabou\",posCaps)\n",
    "    \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementação do modelo\n",
    "\n",
    "Para a implementação do modelo e seu funcionamento, utilizamos diversos métodos de apoio localizados mais adiante neste notebook.\n",
    "\n",
    "A começar pelas dependências de importações: \n",
    "\n",
    "        from numpy.random import choice\n",
    "        import numpy as np\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "A seguir temos um dos métodos mais importantes, o método crossOver(). Nele fazemos a combinação de dois indivíduos (p1 e p2), gerando por fim um novo indivíduo (child).\n",
    "\n",
    "        def crossOver(p1,p2):\n",
    "                dadOrmom = [False, True]\n",
    "                child = []\n",
    "\n",
    "                cross = choice(dadOrmom, p=[0.5,0.5])\n",
    "\n",
    "                if(cross):\n",
    "                        child.append(p1[0])\n",
    "                else:\n",
    "                        child.append(p2[0])\n",
    "                \n",
    "                for i in range(1,10):\n",
    "                        cross = choice(dadOrmom, p=[0.5,0.5])\n",
    "                        if(cross):\n",
    "                        child.append(p1[i])\n",
    "                        else:\n",
    "                        child.append(p2[i])\n",
    "\n",
    "                for i in range(10,18):\n",
    "                        cross = choice(dadOrmom, p=[0.5,0.5])\n",
    "\n",
    "                        if(cross):\n",
    "                \n",
    "                                index = (i-9)*4+9\n",
    "                                child.append(p1[index-3])\n",
    "                                child.append(p1[index-2])\n",
    "                                child.append(p1[index-1])\n",
    "                                child.append(p1[index-0])\n",
    "                        else:\n",
    "                        \n",
    "                                index = (i-9)*4+9\n",
    "                                child.append(p2[index-3])\n",
    "                                child.append(p2[index-2])\n",
    "                                child.append(p2[index-1])\n",
    "                                child.append(p2[index-0])\n",
    "                return child\n",
    "\n",
    "Aqui está um de nosso métodos de apoio, que gera 4 numeros aleatórios cuja soma resulta em 1\n",
    "\n",
    "                def random4():\n",
    "                        p1 = np.random.uniform(low=0.0, high=1)\n",
    "                        p2 = np.random.uniform(low=0.0, high=(1-p1))\n",
    "                        p3 = np.random.uniform(low=0.0, high=(1-p1-p2))\n",
    "                        p4 = 1-(p1+p2+p3)\n",
    "                        while(p1+p2+p3+p4!=1):\n",
    "                                p1 = np.random.uniform(low=0.0, high=1)\n",
    "                                p2 = np.random.uniform(low=0.0, high=(1-p1))\n",
    "                                p3 = np.random.uniform(low=0.0, high=(1-p1-p2))\n",
    "                                p4 = 1-(p1+p2+p3)\n",
    "                        return p1,p2,p3,p4\n",
    "\n",
    "Em sequência temos o método responsável pela mutação do novo indivíduo (child). Nele, cada alelo possui 10% de chance de ser alterado respeitando os valores limites que ele pode assumir.\n",
    "\n",
    "                def mutationChild(child):\n",
    "                        mutation = [False, True]\n",
    "\n",
    "                        ifMut = choice(mutation, p=[0.9,0.1])\n",
    "                        if(ifMut):\n",
    "                                child[0] = np.random.uniform(low=0.0, high=5.0)\n",
    "                        \n",
    "                        for i in range(1,10):\n",
    "                                ifMut = choice(mutation, p=[0.9,0.1)\n",
    "                                if(ifMut):\n",
    "                                if(i == 1):\n",
    "                                        child[i] = random.randint(0, 30)\n",
    "                                else:\n",
    "                                        child[i] = random.randint(0, 23)\n",
    "                        \n",
    "                        for i in range(10,18):\n",
    "                                ifMut = choice(mutation, p=[0.9,0.1)\n",
    "                                if(ifMut):\n",
    "                                \n",
    "                                index = (i-9)*4+9\n",
    "                                p1,p2,p3,p4 = random4()\n",
    "                                child[index-3] = p1\n",
    "                                child[index-2] = p2\n",
    "                                child[index-1] = p3\n",
    "                                child[index-0] = p4\n",
    "                        return child      \n",
    "\n",
    "Temos então mais um método de apoio que nos auxilia na conversão de listas em strings.\n",
    "\n",
    "                def writeString(child):\n",
    "                        string = \"\"\n",
    "                        for i in range(len(child)):\n",
    "                                string += str(child[i])+\" \"\n",
    "                        string+=\"\\n\"\n",
    "                        return string\n",
    "\n",
    "Por fim temos o método geraPopIni, que gera combinações aleatórias de códigos genéticos para serem utilizados como geração inicial do experimento.\n",
    "\n",
    "                def geraPopIni(nome, pop):   \n",
    "                        import numpy as np\n",
    "                        import random\n",
    "                        \n",
    "\n",
    "                        string = \"\" + str(np.random.uniform(low=0.0, high=5.0))\n",
    "\n",
    "                        string += \" \" + str(random.randint(0, 30))\n",
    "\n",
    "                        for i in range(8):\n",
    "                                string += \" \" + str(random.randint(0, 23))\n",
    "\n",
    "                        for i in range(8):\n",
    "                                p1,p2,p3,p4 = random4()\n",
    "                                string += \" \" + str(p1) + \" \" + str(p2) + \" \" + str(p3) + \" \" + str(p4)\n",
    "\n",
    "                        with open('parameters_' + nome + '.txt','w') as f:\n",
    "                                f.write(string)\n",
    "\n",
    "\n",
    "\n",
    "                        for i in range(pop - 1):\n",
    "\n",
    "                                string = \"\\n\" + str(np.random.uniform(low=0.0, high=5.0))\n",
    "\n",
    "                                string += \" \" + str(random.randint(0, 30))\n",
    "\n",
    "                                for i in range(8):\n",
    "                                string += \" \" + str(random.randint(0, 23))\n",
    "\n",
    "                                for i in range(8):\n",
    "                                p1,p2,p3,p4 = random4()\n",
    "                                string += \" \" + str(p1) + \" \" + str(p2) + \" \" + str(p3) + \" \" + str(p4)\n",
    "                                        \n",
    "                                with open('parameters_' + nome + '.txt','a') as f:\n",
    "                                f.write(string)\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execução do experimento\n",
    "\n",
    "Arquitetado todo o aparato necessário para os experimentos, podemos então utilizá-lo num único método que será responsável por reproduzir o que foi mencionado na seção do modelo genético:  \n",
    "* Inicializamos as variáveis necessárias, assim como os arquivos de saída necessários. É gerada também a população inicial do experimento utilizando o método geraPopIni();\n",
    "* Realizamos em loop uma sequência de etapas:\n",
    "    * Para cada indivíduo executamos a simulação com o comando !python pacman.py, que aceita parâmetros para sua execução.\n",
    "    * Feitas as simulações, fazemos a leitura do arquivo de resultados e classificamos os indivíduos a partir de seu fitness.\n",
    "    * Neste ponto é verificado o critério de parada do experimento. Como a simulação exige um certo custo computacional, além do tempo necessário para simulações muito longas, resolvemos restringir o quanto iremos iterar sobre o problema. Resolvemos então adotar um critério que leva em conta as vitórias de um indivíduo, assim como algumas de suas pontuações. Uma vez que um indivíduo ganha um determinado número de jogos, salvamos suas informações num arquivo específico. É dada então uma margem de poucas gerações para que um indivíduo tenha um desempenho melhor e tenha suas informações armazenadas. Restringimos a no máximo 5 gerações, assim como um limite de 10 trocas de melhor indivíduo. Caso nenhum novo melhor indivíduo seja encontrado, ou estejam esgotadas as trocas, encerramos a simulação.\n",
    "    * Caso não seja a hora de encerrar, formamos pares dentre os 5 indivíduos com melhor classificação. A escolha do indivíduo para compor o par é proporcional à sua pontuação. Formamos pares o suficientes para gerar toda uma nova população.\n",
    "    * A cada indivíduo gerado é dada 15% de chance de ele passar pelo processo de mutação através do método mutationChild(), onde cada alelo de seu código genético terá 10% de chance de ser alterado.\n",
    "    \n",
    "    \n",
    "\n",
    "        def treina(layout, nGames, minVic, pop):\n",
    "            from numpy.random import choice\n",
    "            import numpy as np\n",
    "            import time\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            population = pop\n",
    "            count = 6\n",
    "            NotStop = True\n",
    "            hasBest = False\n",
    "\n",
    "            bCount = 0\n",
    "\n",
    "            nome = layout\n",
    "            geraPopIni(nome, pop)\n",
    "\n",
    "            with open('nome.txt', 'w') as f:\n",
    "                f.write(nome)\n",
    "\n",
    "            generation = 0\n",
    "            with open('generation_' + nome + '.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "\n",
    "            with open('fitness_' + nome + '.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "\n",
    "            with open('Best_' + nome + '.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "\n",
    "            while(NotStop):\n",
    "\n",
    "                # Run games\n",
    "\n",
    "                with open('results_' + nome + '.txt','w') as f:\n",
    "                        f.write(\"\")\n",
    "                        f.close()\n",
    "                for i in range(population):\n",
    "\n",
    "                    with open('agentNum_' + nome + '.txt','w') as f:\n",
    "                        f.write(str(i))\n",
    "\n",
    "                    saida = !python pacman.py --layout {layout} --pacman GeneticAgent --numGames {nGames} --quietTextGraphics\n",
    "                    \n",
    "                allPmt = []\n",
    "                allRes = []\n",
    "                allRes2 = []\n",
    "                inds = []\n",
    "\n",
    "                # Read parameters \n",
    "                for k in range (population):\n",
    "                    with open('parameters_' + nome + '.txt') as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                        pmt = lines[k].split()\n",
    "\n",
    "                    for j in range(len(pmt)):\n",
    "                        if(1<=j<=9):\n",
    "                            pmt[j] = int(pmt[j])\n",
    "                        else:\n",
    "                            pmt[j] = float(pmt[j])\n",
    "                    \n",
    "                    allPmt.append(pmt)\n",
    "\n",
    "                # Read results\n",
    "                for k in range (population):\n",
    "                    with open('results_' + nome + '.txt') as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                        res = lines[k].split()\n",
    "\n",
    "                    for j in range(len(res)):\n",
    "                        res[j] = float(res[j])\n",
    "                    \n",
    "                    allRes.append(res)\n",
    "\n",
    "\n",
    "                # Select the best 5\n",
    "\n",
    "                fitList = np.array([])\n",
    "\n",
    "                for k in range (population):\n",
    "                    fit = allRes[k][1]+((229-allRes[k][3])/229)*2 + ((4-allRes[k][4])/4) + (min(allRes[k][5],1))\n",
    "                    fitList = np.append(fitList,fit)\n",
    "\n",
    "                inds = np.argpartition(fitList, -5)[-5:]\n",
    "                indsWorsts = np.argpartition(fitList, 5)[:5]\n",
    "\n",
    "                print(\"Gen: \",generation)\n",
    "                print(\"Best 5:\")\n",
    "                for i in inds:\n",
    "                    print(i,fitList[i],allRes[i])\n",
    "                print(\"Worst 5:\")\n",
    "                for i in indsWorsts:   \n",
    "                    print(i,fitList[i],allRes[i])\n",
    "                print()\n",
    "\n",
    "                avgFit = sum(fitList)/len(fitList)\n",
    "                avgBestFit = 0\n",
    "                avgWorstFit = 0\n",
    "\n",
    "                for i in inds:\n",
    "                    avgBestFit += fitList[i]\n",
    "                avgBestFit = avgBestFit/5\n",
    "                for i in indsWorsts:\n",
    "                    avgWorstFit+= fitList[i]\n",
    "                avgWorstFit = avgWorstFit/5\n",
    "\n",
    "                with open('fitness_' + nome + '.txt','a') as f:\n",
    "                    s = str(avgBestFit) + \" \" + str(avgFit) + \" \" + str(avgWorstFit) + \"\\n\"\n",
    "                    f.write(s)\n",
    "\n",
    "                for i in inds:\n",
    "                    if(allRes[i][1]>=minVic):\n",
    "\n",
    "                        if hasBest:                  \n",
    "\n",
    "                            if fitList[i] > best[0] and allRes[i][0] > best[1] and allRes[i][1] > best[2]:\n",
    "                                best = [fitList[i], allRes[i][0], allRes[i][1]]\n",
    "                                with open('Best_' + nome + '.txt','w') as f:\n",
    "                                    f.write(writeString(allPmt[i]))\n",
    "                                    f.write(writeString([fitList[i])])\n",
    "                                    f.write(writeString(allRes[i]))\n",
    "                                    spentTime = (time.time() - start_time)\n",
    "                                    f.write(\"Execution Time: \"+str(spentTime) + '\\n')\n",
    "                                bCount += 1\n",
    "                                count = 6                  \n",
    "\n",
    "                        else:\n",
    "                            hasBest = True\n",
    "\n",
    "                            best = [fitList[i], allRes[i][0], allRes[i][1]]\n",
    "                            bCount += 1\n",
    "                            with open('Best_' + nome + '.txt','w') as f:\n",
    "                                f.write(writeString(allPmt[i]))\n",
    "                                f.write(writeString([fitList[i])])\n",
    "                                f.write(writeString(allRes[i]))\n",
    "                                spentTime = (time.time() - start_time)\n",
    "                                f.write(\"Execution Time: \"+str(spentTime) + '\\n')\n",
    "                \n",
    "                if (hasBest):\n",
    "                    count -= 1\n",
    "                    if count == 0 or bCount == 10:\n",
    "                        NotStop = False \n",
    "\n",
    "                if(NotStop):\n",
    "\n",
    "                    with open('parameters_' + nome + '.txt','w') as f:\n",
    "                        f.write(\"\")\n",
    "\n",
    "\n",
    "                    # CrossOver\n",
    "\n",
    "                    childs = []\n",
    "\n",
    "                    for k in range(int(population)):               \n",
    "                        d = choice(inds, p=fitList[inds]/fitList[inds].sum())\n",
    "                        m = choice(inds, p=fitList[inds]/fitList[inds].sum())\n",
    "                        while(d==m):\n",
    "                            m = choice(inds, p=fitList[inds]/fitList[inds].sum())\n",
    "\n",
    "\n",
    "                        child = crossOver(allPmt[d],allPmt[m])\n",
    "\n",
    "                        # Mutation\n",
    "                        if choice([True, False], p=[0.15, 0.85]):\n",
    "                            child = mutationChild(child)\n",
    "\n",
    "\n",
    "                        childs.append(child)\n",
    "\n",
    "                    with open('parameters_' + nome + '.txt','a') as f:\n",
    "                        for k in range(population):\n",
    "                            f.write(writeString(childs[k]))\n",
    "\n",
    "                    \n",
    "\n",
    "                    with open('generation_' + nome + '.txt','a') as f:\n",
    "                        f.write(str(generation)+\"\\n\")\n",
    "                    generation+=1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Realizando as simulações\n",
    "\n",
    "Estando tudo pronto, podemos então realizar as simulações nos cenários requisitados pela atividade: smallClassic, mediumClassic e originalClassic.\n",
    "Cada cenário é simulado com parâmetros distintos:\n",
    "\n",
    "* Número de jogos a serem realizados por cada indivíduo;\n",
    "* Número mínimo de vitórias para o critério de parada;\n",
    "* Número de indivíduos das gerações;\n",
    "\n",
    "\n",
    "        nGames = {\"smallClassic\" : 9, \"mediumClassic\" : 6, \"originalClassic\" : 3}\n",
    "        minVic = {\"smallClassic\" : 6, \"mediumClassic\" : 4, \"originalClassic\" : 1}\n",
    "        pops =   {\"smallClassic\" : 200, \"mediumClassic\" : 100, \"originalClassic\" : 50}\n",
    "\n",
    "        for layout in [\"smallClassic\", \"mediumClassic\", \"originalClassic\"]:\n",
    "            treina(layout, nGames[layout], minVic[layout], pops[layout])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Apresentação dos resultados\n",
    "\n",
    "Executadas todas as simulações temos como resultado, para cada geração, as médias dos 5 melhores indivíduos e por fim rodamos nosso melhor indivíduo 10 vezes para cada layout, a média geral da população e por fim a média dos 5 piores indivíduos. Com o seguinte trecho de código imprimimos.\n",
    "\n",
    "        for layout in [\"smallClassic\", \"mediumClassic\", \"originalClassic\"]:\n",
    "            Best = []\n",
    "            Avg = []\n",
    "            Wrst = []\n",
    "            with open('fitness_' + layout + '.txt', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.split()\n",
    "                    Best.append(float(line[0]))\n",
    "                    Avg.append(float(line[1]))\n",
    "                    Wrst.append(float(line[2]))\n",
    "                \n",
    "                x = np.arange(len(lines))\n",
    "            \n",
    "            \n",
    "            plt.plot(x, Best, label = 'Best') \n",
    "            plt.plot(x, Avg, label = 'Average')\n",
    "            plt.plot(x, Wrst, label = 'Worst')\n",
    "            plt.legend()\n",
    "            plt.title(\"Gráfico de Fitness ao longo das gerações\\n* {} *\".format(layout))\n",
    "            plt.xlabel('Geração')\n",
    "            plt.ylabel('Fitness')\n",
    "            plt.show()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layout smallClassic\n",
    "\n",
    "![image](Results/Genetic/Images/graphsmallClassic.png)\n",
    "\n",
    "Pacman died! Score: -411  \n",
    "Pacman emerges victorious! Score: 706  \n",
    "Pacman died! Score: -183  \n",
    "Pacman died! Score: -254  \n",
    "Pacman emerges victorious! Score: 581  \n",
    "Pacman died! Score: -323  \n",
    "Pacman died! Score: -199  \n",
    "Pacman died! Score: -395  \n",
    "Pacman died! Score: -316  \n",
    "Pacman died! Score: -146  \n",
    "Average Score: -94.0  \n",
    "Scores: -411.0, 706.0, -183.0, -254.0, 581.0, -323.0, -199.0, -395.0, -316.0, -146.0  \n",
    "Win Rate: 2/10 (0.20)  \n",
    "Record: Loss, Win, Loss, Loss, Win, Loss, Loss, Loss, Loss, Loss  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layout mediumClassic\n",
    "\n",
    "![image](Results/Genetic/Images/graphmediumClassic.png)\n",
    "\n",
    "Pacman died! Score: -33  \n",
    "Pacman emerges victorious! Score: 799  \n",
    "Pacman died! Score: 8  \n",
    "Pacman died! Score: -186 \n",
    "Pacman emerges victorious! Score: 976  \n",
    "Pacman died! Score: -11  \n",
    "Pacman died! Score: -278  \n",
    "Pacman died! Score: -181  \n",
    "Pacman died! Score: -287  \n",
    "Pacman died! Score: 17  \n",
    "Average Score: 82.4  \n",
    "Scores: -33.0, 799.0, 8.0, -186.0, 976.0, -11.0, -278.0, -181.0, -287.0, 17.0  \n",
    "Win Rate: 2/10 (0.20)  \n",
    "Record: Loss, Win, Loss, Loss, Win, Loss, Loss, Loss, Loss, Loss  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layout originalClassic\n",
    "\n",
    "![image](Results/Genetic/Images/graphoriginalClassic.png)\n",
    "\n",
    "Pacman died! Score: -214  \n",
    "Pacman died! Score: 768  \n",
    "Pacman died! Score: 0  \n",
    "Pacman died! Score: 697  \n",
    "Pacman died! Score: 28  \n",
    "Pacman died! Score: 208  \n",
    "Pacman died! Score: 51  \n",
    "Pacman died! Score: 825  \n",
    "Pacman died! Score: -24  \n",
    "Pacman died! Score: 3  \n",
    "Average Score: 234.2  \n",
    "Scores: -214.0, 768.0, 0.0, 697.0, 28.0, 208.0, 51.0, 825.0, -24.0, 3.0  \n",
    "Win Rate: 0/10 (0.00)  \n",
    "Record: Loss, Loss, Loss, Loss, Loss, Loss, Loss, Loss, Loss, Loss  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusões\n",
    "\n",
    "É possível verificar o resultado esperado em todos os modelos, ao passar da gerações ocorreu o aumento do fitness, principalmente no início há um crescimento rápido, por fim establiza em um valor, porém ao se tratar de probabilidades tanto na criação de uma nova geração quanto na execução e escolha das ações do pacman, é comum surgir alguns melhores outros piores. Podemos ver que mesmo o melhor indíviduo ter ganhado um jogo no modelo original ao rodar ele 10 vezes ele não conseguiu repetir o sucesso, porém apresentando bons scores, uma grande dificuldade encontrada foi quando o pacman, para fugir dos fantasmas, é empurrado para longe de algumas poucas comidas que restaram, como sua exploração se baseia em probabilidade e existe muitos obstáculos, o modelo não apresentou bons resultados nesse estado do pacman muito longe de alguma comida, nos layouts menores essa dificuldade foi amenizada. Uma possibilidade de melhoria seria tentar remodelar para se basear menos em probabilidade em suas ações, considerando também o movimento estocástico dos fantasmas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para o aprendizado por reforço foi escolhido o método Q-Learning para ser utilizado. Um método model-free e value-based, onde inicialmente iremos definir uma Q-Table ($Q[S, A]$) com conjunto de estados e ações e atualizando o valor de qualidade para cada $Q(s,a)$ ao decorrer do treinamento através de recompensa e a Q-function. A seguir será explicado a teoria utilizada e o modelo a ser implementado, após isso será apresentado o código e explicações das funções mais relevantes e por fim os gráficos gerados e conclusões do processo. Este segmento, assim como o modelo genético, não poderá ser executado, apesar de pré-gerados os códigos e resultados serão todos apresentados em markdown."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Passos do Q-Learning\n",
    "\n",
    "## Passo 1: Q-Table  \n",
    "\n",
    "A tabela utilizada no Q-Learning é uma combinação de estados $s$ do conjunto $S$ e ações $a$ do conjunto $A$, sendo assim, cada $(s,a)$ possui um valor de qualidade dado por $Q(s,a)$.   \n",
    "  \n",
    "O ideal seria utilizar o estado completo do jogo, porém para poupar poder computacional, construímos os estados baseado em quais ações são permitidas na atual posição do pacman (Norte, Sul, Leste e Oeste), se possui algum fantasma perto (menor que 3.5 unidades de distância), posições relativas do fantasma e da comida mais próxima (Norte, Sul, Leste, Oeste, Nordeste, Sudeste, Noroeste e Sudoeste). O conjunto de ações consiste em Norte, Sul, Leste e Oeste.  \n",
    "  \n",
    "Sendo assim, obteremos uma tabela com a coluna sendo os estados e cada linha sendo as ações. O modelo irá analisar a cada nova posição do pacman qual estado ele está, se ainda não estiver na tabela ele é incluído e todos os elementos terão valor inicialmente 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 2: Escolhendo e perfomando a ação\n",
    "\n",
    "A cada estado deve-se escolher uma ação para depois ser avaliada e o intuito é escolher a ação que maximizará o valor de Q para o estado atual, porém deve-se balancear o exploration e exploitation do agente, sendo assim o modelo possui um $\\epsilon = 0.1$ que o fornece a probabilidade de 10% de escolher uma ação aleatória permitida naquele estado, evitando assim que sempre repita uma ação, mesmo que boa, pois há a possibilidade de uma melhor, ainda não explorada, existir."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 3: Medindo a recompensa\n",
    "\n",
    "Para avaliar a ação e atualizar o valor em $Q(s,a)$ precisa-se definir uma recompensa para ação. A função de recompensa $R(s,a)$ para esse modelo se baseia no seguinte algoritmo:\n",
    "* Se possuía um fantasma próximo:\n",
    "    * Se a distância aumentou +30 pontos\n",
    "    * Senão -30 pontos\n",
    "* Senão:\n",
    "    * Se comeu comida ou cápsula:\n",
    "        * Se comeu comida +10 pontos\n",
    "        * Se comeu cápsula +20 pontos\n",
    "    * Senão:\n",
    "        * Se aproximou da comida +15 pontos\n",
    "        * Senão -15 pontos\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 4: Atualizando a Q-Table\n",
    "\n",
    "Cada elemento da Q-Table será atualizada através da equação de Bellman:  \n",
    "  \n",
    "$New Q(s,a) = Q(s,a) + \\alpha(R(s,a) + \\gamma maxQ(s,a) - Q(s,a))$  \n",
    "  \n",
    "Onde:  \n",
    "$New Q(s,a)$ é o novo elemento em $Q(s,a)$.  \n",
    "$Q(s,a)$ é o valor atual em Q-Table.  \n",
    "$\\alpha$ é o learning rate, nosso modelo será 0.2.  \n",
    "$R(s,a)$ é a função de recompensa.  \n",
    "$\\gamma$ é o discount rate, nosso modelo será 0.8.  \n",
    "$maxQ(s,a)$ é o maior Q-value associado a este estado.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 5: Repetir os passos 2,3 e 4\n",
    "\n",
    "O modelo deverá executar diversos episódios até aprender, idealmente poderíamos definir um critério de parada para o algoritmo, porém como iremos passar o número de treinos a ser realizado por linha de comando tivemos dificuldade em definir um critério de parada, sendo assim, para o layout de smallClassic será executado 10 mil episódios, para o mediumClassic 5 mil e o originalClassic 2mil. O número de treinos foi se reduzindo por limitações computacionais."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementação do Q-learning\n",
    "\n",
    "A implementação segue o seguinte algoritmo:\n",
    "\n",
    "* No construtor:\n",
    "    * Inicializa um dicionário para armazenar $Q[S,A]$\n",
    "    * Inicializa os parâmetros utilizados na Q-Function\n",
    "    * Inicializa atributos utilizados para avaliar a recompensa  \n",
    "  \n",
    "  \n",
    "* Função getAction():\n",
    "    * Analisa qual o estado atual\n",
    "    * Avalia a recompensa\n",
    "    * Atualiza Q-Table\n",
    "    * Atualiza os atributos da classe\n",
    "    * Escolhe uma nova ação\n",
    "  \n",
    "\n",
    "* Função final():\n",
    "    * Analisa qual o estado atual\n",
    "    * Avalia a recompensa\n",
    "    * Atualiza Q-Table \n",
    "    * Reseta os atributos da classe\n",
    "    * Finaliza o episódio\n",
    "\n",
    "\n",
    "No arquivo Agents.py possui a implementação da classe QLearnAgent que será apresentada e explicada aqui. Segue abaixo o código completo e a explicação para as principais funções"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicialmente temos o construtor da classe incializando a Q-Table com um dicionário e definindo outros atributos que serão utilizados.\n",
    "\n",
    "```python\n",
    "class QLearnAgent(Agent):\n",
    "\n",
    "    # Inicializa o Agent com os atributos necessários\n",
    "    def __init__(self, numTraining = 10):\n",
    "\n",
    "        # Parâmetros do Q-Learning \n",
    "        self.alpha = 0.2\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.8\n",
    "        self.qValues = dict()\n",
    "\n",
    "        # Acompanha resultados durante o episódio\n",
    "        self.numTraining = int(numTraining)\n",
    "        self.episodesSoFar = 0\n",
    "        self.actionsSoFar = 0\n",
    "        self.totalReward = 0\n",
    "        \n",
    "        # Usado para criar estado e recompensas\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.lastScore = None\n",
    "        self.lastNumFood = None\n",
    "        self.lastCaps = None\n",
    "        self.lastDistGhost = None\n",
    "        self.lastDistFood = None\n",
    "        self.doNotEat = None\n",
    "        self.ghostWasNear = False\n",
    "        self.lastFoodPosition = None\n",
    "\n",
    "        # Escreve em arquivos diferente dependendo do número de treino\n",
    "        if(self.getNumTraining() < 2500):\n",
    "            with open('episodesResults3.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "        elif(self.getNumTraining() < 5500):\n",
    "            with open('episodesResults2.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "        else:\n",
    "            with open('episodesResults1.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "```\n",
    "A seguir são algumas funções para acessar, setar e incrementar os atributos.\n",
    "\n",
    "```python\n",
    "\n",
    "    # Funções para acessar os atributos\n",
    "\n",
    "    def incrementEpisodesSoFar(self):\n",
    "        self.episodesSoFar += 1\n",
    "\n",
    "    def getEpisodesSoFar(self):\n",
    "        return self.episodesSoFar\n",
    "    \n",
    "    def incrementAcionsSoFar(self):\n",
    "        self.actionsSoFar += 1\n",
    "    \n",
    "    def getAcionsSoFar(self):\n",
    "        return self.actionsSoFar\n",
    "\n",
    "    def getNumTraining(self):\n",
    "        return self.numTraining\n",
    "\n",
    "    def setEpsilon(self, value):\n",
    "        self.epsilon = value\n",
    "\n",
    "    def getAlpha(self):\n",
    "        return self.alpha\n",
    "\n",
    "    def setAlpha(self, value):\n",
    "        self.alpha = value\n",
    "\n",
    "    def getGamma(self):\n",
    "        return self.gamma\n",
    "```\n",
    "\n",
    "A função seguinte recebe o estado do jogo e os movimentos legais do pacman e retorna uma string descrevendo o estado do conjunto S\n",
    "\n",
    "```python\n",
    "    # Cria o estado baseado em direções legais, distância do fantasma mais próximo e sua posição relativa e da comida mais próxima\n",
    "    def createState(self,state,legal):\n",
    "        posG = np.array(state.getGhostPositions())\n",
    "        posP = np.array(state.getPacmanPosition())\n",
    "        posFood = state.getFood()\n",
    "        numFood = state.getNumFood()\n",
    "\n",
    "        minDistGhost = 99999\n",
    "        indG = None\n",
    "        for i in range(len(posG)):\n",
    "            if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "                indG =  i\n",
    "\n",
    "        if(posG[indG][0] > posP[0]):\n",
    "            if(posG[indG][1] > posP[1]):\n",
    "                ghostRelativePosition = \"upright\"\n",
    "            elif(posG[indG][1] == posP[1]):\n",
    "                ghostRelativePosition = \"right\"\n",
    "            else:\n",
    "                ghostRelativePosition = \"downright\"\n",
    "\n",
    "        elif(posG[indG][0] == posP[0]):\n",
    "            if(posG[indG][1] > posP[1]):\n",
    "                ghostRelativePosition = \"up\"\n",
    "            else:\n",
    "                ghostRelativePosition = \"down\"\n",
    "        else:\n",
    "            if(posG[indG][1] > posP[1]):\n",
    "                ghostRelativePosition = \"upleft\"\n",
    "            elif(posG[indG][1] == posP[1]):\n",
    "                ghostRelativePosition = \"left\"\n",
    "            else:\n",
    "                ghostRelativePosition = \"downleft\"\n",
    "\n",
    "        if(numFood > 0):\n",
    "            minDistFood = 99999\n",
    "            minFood = None\n",
    "            for x in range(posFood.width):\n",
    "                for y in range(posFood.height):\n",
    "                    if(posFood[x][y]):\n",
    "                        food = np.array([x,y])\n",
    "                        distFood = np.linalg.norm(food-posP)\n",
    "                        if(distFood<minDistFood):\n",
    "                            minDistFood = distFood\n",
    "                            minFood = food\n",
    "            \n",
    "            if(minFood[0] > posP[0]):\n",
    "                if(minFood[1] > posP[1]):\n",
    "                    foodRelativePosition = \"upright\"\n",
    "                elif(minFood[1] == posP[1]):\n",
    "                    foodRelativePosition = \"right\"\n",
    "                else:\n",
    "                    foodRelativePosition = \"downright\"\n",
    "\n",
    "            elif(minFood[0] == posP[0]):\n",
    "                if(minFood[1] > posP[1]):\n",
    "                    foodRelativePosition = \"up\"\n",
    "                else:\n",
    "                    foodRelativePosition = \"down\"\n",
    "            else:\n",
    "                if(minFood[1] > posP[1]):\n",
    "                    foodRelativePosition = \"upleft\"\n",
    "                elif(minFood[1] == posP[1]):\n",
    "                    foodRelativePosition = \"left\"\n",
    "                else:\n",
    "                    foodRelativePosition = \"downleft\"\n",
    "            self.lastFoodPosition  = foodRelativePosition\n",
    "        else:\n",
    "            foodRelativePosition = self.lastFoodPosition\n",
    "\n",
    "        if(minDistGhost < 3):\n",
    "            ghostNear = True\n",
    "        else:\n",
    "            ghostNear = False\n",
    "        \n",
    "        # Retorna uma string descrevendo o estado\n",
    "        return str(legal)+str(ghostNear)+str(ghostRelativePosition)+str(foodRelativePosition)\n",
    "```\n",
    "A função seguinte inicializa um novo Q(s,a)\n",
    "\n",
    "```python\n",
    "\n",
    "    # Caso o estado ainda não exista é gerado um para cada ação legal com valor Q = 0\n",
    "    def initializeQValues(self, pacmanState, legal):\n",
    "        self.qValues[pacmanState] = dict()\n",
    "        for action in legal:\n",
    "            if action not in self.qValues[pacmanState]:\n",
    "                self.qValues[pacmanState][action] = 0.0\n",
    "```\n",
    "A seguinte função é utilizada para avaliar a recompensa da última ação e atualizar o valor na Q-Table\n",
    "```python\n",
    "\n",
    "    # Calcula a recompensa da última ação e atualiza  valor Q do estado e sua ação através da função do Q-learning\n",
    "    def updateQValue(self, state,pacmanState, final_step=False):\n",
    "        \n",
    "        posG = np.array(state.getGhostPositions())\n",
    "        posP = np.array(state.getPacmanPosition())\n",
    "        posFood = state.getFood()\n",
    "\n",
    "        numFoodEat = self.lastNumFood - state.getNumFood()\n",
    "        numCapsEat = self.lastCaps - len(state.getCapsules())\n",
    "\n",
    "        minDistGhost = 99999\n",
    "        for i in range(len(posG)):\n",
    "            if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "        \n",
    "        minDistFood = 99999\n",
    "        for x in range(posFood.width):\n",
    "            for y in range(posFood.height):\n",
    "                if(posFood[x][y]):\n",
    "                    food = np.array([x,y])\n",
    "                    distFood = np.linalg.norm(food-posP)\n",
    "                    if(distFood<minDistFood):\n",
    "                        minDistFood = distFood\n",
    "\n",
    "        if(self.ghostWasNear):\n",
    "            if(minDistGhost >= self.lastDistGhost):\n",
    "                didNotEat = 30\n",
    "            else:\n",
    "                didNotEat = -30\n",
    "        else:\n",
    "            if(numFoodEat == 0 or numCapsEat == 0):\n",
    "                if(minDistFood > self.lastDistFood):\n",
    "                    didNotEat = -15\n",
    "                else:\n",
    "                    didNotEat = 15\n",
    "            else:\n",
    "                didNotEat = 0\n",
    "\n",
    "        reward = numFoodEat * 10 + numCapsEat * 20 + didNotEat\n",
    "        self.totalReward += reward\n",
    "\n",
    "        max_Q_value = 0.0\n",
    "        if not final_step:\n",
    "            max_Q_value = max(list(self.qValues[pacmanState].values()))\n",
    "        self.qValues[self.lastState][self.lastAction] += (self.alpha * (reward + self.gamma * max_Q_value - self.qValues[self.lastState][self.lastAction]))\n",
    "```\n",
    "A função seguinte escolhe uma nova ação, com possibilidade de ser uma ação aleatória.\n",
    "\n",
    "```python\n",
    "\n",
    "    # Escolha a nova ação balanceando exploration e exploitation\n",
    "    def epsilonGreedy(self, state,pacmanState, legal):\n",
    "        if Directions.STOP in legal:\n",
    "            legal.remove(Directions.STOP)\n",
    "\n",
    "        if not self.ghostWasNear:\n",
    "            if(self.lastAction!= None):\n",
    "                if (Directions.REVERSE[self.lastAction] in legal) and len(legal)>1:\n",
    "                    legal.remove(Directions.REVERSE[self.lastAction])\n",
    "\n",
    "        probability = random.random()\n",
    "\n",
    "        if probability < self.epsilon: # Escolhe ação aleatória\n",
    "            random_action = random.choice(legal)\n",
    "            return random_action\n",
    "\n",
    "        max_Q_action = None # Escolhe a que já sabe ser melhor\n",
    "        for action in legal:\n",
    "            if max_Q_action == None:\n",
    "                max_Q_action = action\n",
    "            if self.qValues[pacmanState][action] > self.qValues[pacmanState][max_Q_action]:\n",
    "                max_Q_action = action\n",
    "        return max_Q_action\n",
    "```\n",
    "As duas funções a seguir são somente utilizadas para atualizar ou resetar os atributos da classe\n",
    "\n",
    "```python\n",
    "\n",
    "    # Atualiza atributos para um novo estado\n",
    "    def updateAttributes(self, state,pacmanState, legal):\n",
    "\n",
    "        posG = np.array(state.getGhostPositions())\n",
    "        posP = np.array(state.getPacmanPosition())\n",
    "        posFood = state.getFood()\n",
    "\n",
    "        self.lastNumFood = state.getNumFood()\n",
    "        self.lastCaps = len(state.getCapsules())\n",
    "\n",
    "        minDistGhost = 99999\n",
    "        for i in range(len(posG)):\n",
    "            if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "        \n",
    "        minDistFood = 99999\n",
    "        for x in range(posFood.width):\n",
    "            for y in range(posFood.height):\n",
    "                if(posFood[x][y]):\n",
    "                    food = np.array([x,y])\n",
    "                    distFood = np.linalg.norm(food-posP)\n",
    "                    if(distFood<minDistFood):\n",
    "                        minDistFood = distFood\n",
    "        \n",
    "        if(minDistGhost <= 3.5 ):\n",
    "            self.ghostWasNear = True\n",
    "        else:\n",
    "            self.ghostWasNear = False\n",
    "        self.lastDistGhost = minDistGhost\n",
    "        self.lastDistFood = minDistFood\n",
    "        self.lastState = self.createState(state,legal)\n",
    "        self.lastAction = self.epsilonGreedy(state,pacmanState, legal)\n",
    "        self.lastScore = state.getScore()\n",
    "\n",
    "    # Reseta todos atributos (no fim do episódio)\n",
    "    def resetAttributes(self):\n",
    "        self.totalReward = 0\n",
    "        self.actionsSoFar = 0\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.lastScore = None\n",
    "```\n",
    "\n",
    "A função getAction é chamada a cada passo do pacman e realiza os passos do Q-Learning utilizando as funções já implementadas\n",
    "\n",
    "```python\n",
    "\n",
    "    # Escolhe uma ação, inicializa o estado que o pacman está caso não exista ainda e move o pacman\n",
    "    def getAction(self, state):\n",
    "\n",
    "        legal = state.getLegalPacmanActions()\n",
    "\n",
    "        if Directions.STOP in legal: # Remove a ação de parar\n",
    "            legal.remove(Directions.STOP)\n",
    "        \n",
    "        if not self.ghostWasNear: # Se não possuir fantasmas próximos impede o pacman reverter\n",
    "            if(self.lastAction!= None):\n",
    "                if (Directions.REVERSE[self.lastAction] in legal) and len(legal)>1:\n",
    "                    legal.remove(Directions.REVERSE[self.lastAction])\n",
    "        \n",
    "        pacmanState = self.createState(state,legal) # Analisa o estado atual\n",
    "\n",
    "        if pacmanState not in self.qValues: # Insere o estado na Q-Table\n",
    "            self.initializeQValues(pacmanState, legal)\n",
    "\n",
    "        if self.lastState != None: # Avalia a recompensa e atualiza Q-Table\n",
    "            self.updateQValue(state,pacmanState)\n",
    "\n",
    "        self.updateAttributes(state,pacmanState, legal) # Atualiza os atributos da classe\n",
    "\n",
    "        self.incrementAcionsSoFar()\n",
    "\n",
    "        return self.lastAction\n",
    "```\n",
    "A função final é chamada após uma derrota ou vitória encerrando o episódio\n",
    "\n",
    "```python\n",
    "\n",
    "    # Chamada no fim do episódio e atualiza o último estágio\n",
    "    def final(self, state):\n",
    "        legal = state.getLegalPacmanActions()\n",
    "\n",
    "        if Directions.STOP in legal:\n",
    "            legal.remove(Directions.STOP)\n",
    "        \n",
    "        if not self.ghostWasNear:\n",
    "            if(self.lastAction!= None):\n",
    "                if (Directions.REVERSE[self.lastAction] in legal) and len(legal)>1:\n",
    "                    legal.remove(Directions.REVERSE[self.lastAction])\n",
    "\n",
    "        if self.lastState != None:\n",
    "            self.updateQValue(state,self.createState(state,legal), final_step=True)\n",
    "\n",
    "\n",
    "        if(self.getNumTraining() < 2500):\n",
    "            with open('episodesResults3.txt','a') as f:\n",
    "                f.write(str(self.totalReward)+\" \"+str(self.getAcionsSoFar())+\" \"+str(state.getScore())+\"\\n\")\n",
    "        elif(self.getNumTraining() < 5500):\n",
    "            with open('episodesResults2.txt','a') as f:\n",
    "                f.write(str(self.totalReward)+\" \"+str(self.getAcionsSoFar())+\" \"+str(state.getScore())+\"\\n\")\n",
    "        else:\n",
    "            with open('episodesResults1.txt','a') as f:\n",
    "                f.write(str(self.totalReward)+\" \"+str(self.getAcionsSoFar())+\" \"+str(state.getScore())+\"\\n\")\n",
    "\n",
    "        self.resetAttributes()\n",
    "\n",
    "        self.incrementEpisodesSoFar()\n",
    "\n",
    "        if self.getEpisodesSoFar() == self.getNumTraining():\n",
    "            self.setAlpha(0)\n",
    "            self.setEpsilon(0)\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execução do Q-Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para executar os treinos foi utilizado o seguinte código num notebook, o tempo de execução foi 1.84 horas.  \n",
    "Assim foi escrito em arquivos separados os resultados de cada episódio e também os resultados das 10 execuções dos melhores agentes.\n",
    "\n",
    "\n",
    "``` python\n",
    "saida1 = !python pacman.py -p QLearnAgent -x 10000 -n 10010 -l smallClassic \n",
    "with open('results1.txt','w') as f:\n",
    "    for i in saida1:\n",
    "        f.write(str(i)+'\\n')\n",
    "\n",
    "saida2 = !python pacman.py -p QLearnAgent -x 5000 -n 5010 -l mediumClassic \n",
    "with open('results2.txt','w') as f:\n",
    "    for i in saida2:\n",
    "        f.write(str(i)+'\\n')\n",
    "\n",
    "saida3 = !python pacman.py -p QLearnAgent -x 2000 -n 2010 -l originalClassic \n",
    "with open('results3.txt','w') as f:\n",
    "    for i in saida3:\n",
    "        f.write(str(i)+'\\n')\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A seguir segue o código excutado para gerar os gráficos\n",
    "\n",
    "``` python\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "def createGraphs(file,title,YLabel,EpNum,ind,fileGenerated):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        xy = [[int(line.split()[0]),int(line.split()[1]),float(line.split()[2])] for line in lines]\n",
    "\n",
    "    epResults = []\n",
    "    for i in range(len(xy[0])):  \n",
    "        aux1 = []\n",
    "        eps = []\n",
    "        for j in range(len(xy)):\n",
    "            eps.append(j+1)\n",
    "            aux1.append(xy[j][i])\n",
    "        epResults.append(aux1)\n",
    "\n",
    "    fig = plt.figure(figsize=(60, 6))\n",
    "    ax = fig.add_subplot()\n",
    "    ax = plt.axes()\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1000))\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(250))\n",
    "    plt.xlim(0, EpNum)\n",
    "    plt.scatter(eps,epResults[ind])\n",
    "    plt.tick_params(axis='x', which='major', labelsize=30)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=30)\n",
    "    plt.title(title,fontsize=30)\n",
    "    plt.xlabel('Episodes',fontsize=30)\n",
    "    plt.ylabel(YLabel,fontsize=30)\n",
    "    plt.savefig(fileGenerated)\n",
    "\n",
    "createGraphs(\"episodesResults1.txt\",\"Reward x Episodes smallClassic\",\"Total Reward\",10000,0,\"TRsmall.png\")\n",
    "createGraphs(\"episodesResults1.txt\",\"Actions x Episodes smallClassic\",\"Total Actions\",10000,1,\"TAsmall.png\")\n",
    "createGraphs(\"episodesResults1.txt\",\"Score x Episodes smallClassic\",\"Total Score\",10000,2,\"TSsmall.png\")\n",
    "\n",
    "createGraphs(\"episodesResults2.txt\",\"Reward x Episodes mediumClassic\",\"Total Reward\",5000,0,\"TRmedium.png\")\n",
    "createGraphs(\"episodesResults2.txt\",\"Actions x Episodes mediumClassic\",\"Total Actions\",5000,1,\"TAmedium.png\")\n",
    "createGraphs(\"episodesResults2.txt\",\"Score x Episodes mediumClassic\",\"Total Score\",5000,2,\"TSmedium.png\")\n",
    "\n",
    "createGraphs(\"episodesResults3.txt\",\"Reward x Episodes originalClassic\",\"Total Reward\",2000,0,\"TRoriginal.png\")\n",
    "createGraphs(\"episodesResults3.txt\",\"Actions x Episodes originalClassic\",\"Total Actions\",2000,1,\"TAoriginal.png\")\n",
    "createGraphs(\"episodesResults3.txt\",\"Score x Episodes originalClassic\",\"Total Score\",2000,2,\"TSoriginal.png\")\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resultados do Q-Learning\n",
    "\n",
    "## Layout smallClassic\n",
    "\n",
    "![image](Results/Reinforcement/Images/TRsmall.png)\n",
    "![image](Results/Reinforcement/Images/TAsmall.png)\n",
    "![image](Results/Reinforcement/Images/TSsmall.png)\n",
    "\n",
    "Pacman emerges victorious! Score: 921  \n",
    "Pacman died! Score: -347  \n",
    "Pacman emerges victorious! Score: 1137  \n",
    "Pacman emerges victorious! Score: 893  \n",
    "Pacman died! Score: -214  \n",
    "Pacman emerges victorious! Score: 967  \n",
    "Pacman died! Score: -109  \n",
    "Pacman emerges victorious! Score: 938  \n",
    "Pacman emerges victorious! Score: 931  \n",
    "Pacman emerges victorious! Score: 884  \n",
    "Average Score: 600.1  \n",
    "Scores: 921.0, -347.0, 1137.0, 893.0, -214.0, 967.0, -109.0, 938.0, 931.0, 884.0  \n",
    "Win Rate: 7/10 (0.70)  \n",
    "Record: Win, Loss, Win, Win, Loss, Win, Loss, Win, Win, Win  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layout mediumClassic\n",
    "\n",
    "![image](Results/Reinforcement/Images/TRmedium.png)\n",
    "![image](Results/Reinforcement/Images/TAmedium.png)\n",
    "![image](Results/Reinforcement/Images/TSmedium.png)\n",
    "\n",
    "Pacman emerges victorious! Score: 1257  \n",
    "Pacman emerges victorious! Score: 1320  \n",
    "Pacman emerges victorious! Score: 1272  \n",
    "Pacman emerges victorious! Score: 1462  \n",
    "Pacman died! Score: 171  \n",
    "Pacman emerges victorious! Score: 1247  \n",
    "Pacman emerges victorious! Score: 1270  \n",
    "Pacman emerges victorious! Score: 1317  \n",
    "Pacman died! Score: -126  \n",
    "Pacman died! Score: 72  \n",
    "Average Score: 926.2  \n",
    "Scores: 1257.0, 1320.0, 1272.0, 1462.0, 171.0, 1247.0, 1270.0, 1317.0, -126.0, 72.0  \n",
    "Win Rate: 7/10 (0.70)  \n",
    "Record:  Win, Win, Win, Win, Loss, Win, Win, Win, Loss, Loss "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layout originalClassic\n",
    "\n",
    "![image](Results/Reinforcement/Images/TRoriginal.png)\n",
    "![image](Results/Reinforcement/Images/TAoriginal.png)\n",
    "![image](Results/Reinforcement/Images/TSoriginal.png)\n",
    "\n",
    "Pacman died! Score: 392  \n",
    "Pacman emerges victorious! Score: 2297  \n",
    "Pacman died! Score: 615  \n",
    "Pacman died! Score: 831  \n",
    "Pacman died! Score: 976  \n",
    "Pacman died! Score: 293  \n",
    "Pacman died! Score: 762  \n",
    "Pacman emerges victorious! Score: 2302  \n",
    "Pacman died! Score: 885  \n",
    "Pacman died! Score: 402  \n",
    "Average Score: 975.5  \n",
    "Scores: 392.0, 2297.0, 615.0, 831.0, 976.0, 293.0, 762.0, 2302.0, 885.0, 402.0  \n",
    "Win Rate: 2/10 (0.20)  \n",
    "Record: Loss, Win, Loss, Loss, Loss, Loss, Loss, Win, Loss, Loss "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusões\n",
    "\n",
    "A visualização dos gráficos não ficou tão clara quanto do método genético, porém é possível evidenciar as seguintes observações. A recompensa total por episódio cresce com bons resultados logo no início, como não utilizamos um critério de parada para utilizar o numTraining possivelmente o metódo teria convergido antes do número total de execuções utilizado. Outra observação relevante é verificar que, apesar de ter episódios com um grande número de ações, não encontra-se um bom reward, muitas vezes por o pacman ter entrado em algum loop realizando ações sem valor. Por fim vale ressaltar no gráfico de score, esxite uma lacuna separando valores na parte superior e inferior do gráfico, como o pacman recebe uma grande recompensa de score ao obter vitória e uma grande penalização de escore ao obter derrota, essa lacuna está separando episódios vitoriosos de derrotas, e é possível verificar que a concentração de pontos na zona superior aumenta ao passar dos episódios.  \n",
    "\n",
    "A grande dificuldade que atingiu esse método, também o genético, porém nesse o resultado foi mais visível, foi o pacman estar em estados semelhantes ao apresentado abaixo, o nosso modelo não prevê como evitar essas situações impossíveis de sair. \n",
    "\n",
    "<img src=\"Results/Reinforcement/Images/ImpossibleScape.png\" alt=\"img\" width=\"200\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparação de ambos metódos\n",
    "\n",
    "## Resultados\n",
    "\n",
    "* smallClassic\n",
    "    * Reforço:\n",
    "        * Average Score: 600.1\n",
    "        * Win Rate: 7/10 (0.70)\n",
    "    * Genético:\n",
    "        * Average Score: -94.0 \n",
    "        * Win Rate: 2/10 (0.20)\n",
    "  \n",
    "* mediumClassic\n",
    "    * Reforço:\n",
    "        * Average Score: 926.2  \n",
    "        * Win Rate: 7/10 (0.70)\n",
    "    * Genético:\n",
    "        * Average Score: 82.4 \n",
    "        * Win Rate: 2/10 (0.20)\n",
    "  \n",
    "* originalClassic\n",
    "    * Reforço:\n",
    "        * Average Score: 975.5   \n",
    "        * Win Rate: 2/10 (0.20)\n",
    "    * Genético:\n",
    "        * Average Score: 234.2 \n",
    "        * Win Rate: 0/10 (0.00)\n",
    "\n",
    "## Conclusões finais \n",
    "\n",
    "Fica claro como os resultados apresentados pelo aprendizado por reforço utilizando Q-Learning foram superiores, muito se deve por a escolha de ação do pacman não ser probabilístico e esse método armazenar mais estados para escolher ações, porém esse aspecto presente no nosso modelo genético não é intrinsíco do mesmo, pode ser alterado para apresentar melhorias levando a outro ponto de diferenciação de ambos os métodos, aprendizado por reforço foi encontrado muitos métodos com uma implementação direta e fácil adaptação para o pacman, como o Q-Learning utilizado, que buscam soluções ótimas. Enquanto no método genético exigiu uma maior criatividade ao modelar o comportamento do pacman e parâmetros a ser utilizado, semelhante a definir estados no Q-Learning, porém no genético encontramos uma dificuldade maior, possivelmente disso gerou resultados inferiores. Com isso vale ressaltar que o genético foi o primeiro método a ser implementado e com ele aprendemos e entendemos melhor como utilizar o framework do pacman e foram surgindo mais idéias que contribuiram para a implementação do Q-Learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contribuições individuais\n",
    "\n",
    "Heigon Soldera 217638 - Início do método genético, modelagem e implementação do Q-Learning   \n",
    "Rafael Galib 204904 - Finalização do método genético, gerar gráficos\n",
    "\n",
    "Muitas contribuições foram realizadas simultaneamente a partir de discussões e implementação conjunta através de chamadas online."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "1836012103c67e819a23e4734bf3459f03aa3ddd3450d92a135c42dd7ae4103a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}