{
 "cells": [
  {
   "source": [
    "# Reinforcement Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " Para o aprendizado por reforço foi escolhido o método Q-Learning para ser utilizado. Um método model-free e value-based, onde inicialmente iremos definir uma Q-Table ($Q[S, A]$) com conjunto de estados e ações e atualizando o valor de qualidade para cada $Q(s,a)$ ao decorrer do treinamento através de recompensa e a Q-function. A seguir será explicado a teoria utilizada e o modelo a ser implementado, após isso será apresentado o código e explicações das funções mais relevantes e por fim os gráficos gerados e conclusões do processo. Este segmento, assim como o modelo genético, não poderá ser executado, apesar de pré-gerados os códigos e resultados serão todos apresentados em markdown."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Passos do Q-Learning\n",
    "\n",
    "## Passo 1: Q-Table  \n",
    "\n",
    "A tabela utilizada no Q-Learning é uma combinação de estados $s$ do conjunto $S$ e ações $a$ do conjunto $A$, sendo assim, cada $(s,a)$ possui um valor de qualidade dado por $Q(s,a)$.   \n",
    "  \n",
    "O ideal seria utilizar o estado completo do jogo, porém para poupar poder computacional, construímos os estados baseado em quais ações são permitidas na atual posição do pacman (Norte, Sul, Leste e Oeste), se possui algum fantasma perto (menor que 3.5 unidades de distância), posições relativas do fantasma e da comida mais próxima (Norte, Sul, Leste, Oeste, Nordeste, Sudeste, Noroeste e Sudoeste). O conjunto de ações consiste em Norte, Sul, Leste e Oeste.  \n",
    "  \n",
    "Sendo assim, obteremos uma tabela com a coluna sendo os estados e cada linha sendo as ações. O modelo irá analisar a cada nova posição do pacman qual estado ele está, se ainda não estiver na tabela ele é incluído e todos os elementos terão valor inicialmente 0.\n",
    "  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Passo 2: Escolhendo e perfomando a ação\n",
    "\n",
    "A cada estado deve-se escolher uma ação para depois ser avaliada e o intuito é escolher a ação que maximizará o valor de Q para o estado atual, porém deve-se balancear o exploration e exploitation do agente, sendo assim o modelo possui um $\\epsilon = 0.1$ que o fornece a probabilidade de 10% de escolher uma ação aleatória permitida naquele estado, evitando assim que sempre repita uma ação, mesmo que boa, pois há a possibilidade de uma melhor, ainda não explorada, existir.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Passo 3: Medindo a recompensa\n",
    "\n",
    "Para avaliar a ação e atualizar o valor em $Q(s,a)$ precisa-se definir uma recompensa para ação. A função de recompensa $R(s,a)$ para esse modelo se baseia no seguinte algoritmo:\n",
    "* Se possuía um fantasma próximo:\n",
    "    * Se a distância aumentou +30 pontos\n",
    "    * Senão -30 pontos\n",
    "* Senão:\n",
    "    * Se comeu comida ou cápsula:\n",
    "        * Se comeu comida +10 pontos\n",
    "        * Se comeu cápsula +20 pontos\n",
    "    * Senão:\n",
    "        * Se aproximou da comida +15 pontos\n",
    "        * Senão -15 pontos\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Passo 4: Atualizando a Q-Table\n",
    "\n",
    "Cada elemento da Q-Table será atualizada através da equação de Bellman:  \n",
    "  \n",
    "$New Q(s,a) = Q(s,a) + \\alpha(R(s,a) + \\gamma maxQ(s,a) - Q(s,a))$  \n",
    "  \n",
    "Onde:  \n",
    "$New Q(s,a)$ é o novo elemento em $Q(s,a)$.  \n",
    "$Q(s,a)$ é o valor atual em Q-Table.  \n",
    "$\\alpha$ é o learning rate, nosso modelo será 0.2.  \n",
    "$R(s,a)$ é a função de recompensa.  \n",
    "$\\gamma$ é o discount rate, nosso modelo será 0.8.  \n",
    "$maxQ(s,a)$ é o maior Q-value associado a este estado.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Passo 5: Repetir os passos 2,3 e 4\n",
    "\n",
    "O modelo deverá executar diversos episódios até aprender, idealmente poderíamos definir um critério de parada para o algoritmo, porém como iremos passar o número de treinos a ser realizado por linha de comando tivemos dificuldade em definir um critério de parada, sendo assim, para o layout de smallClassic será executado 10 mil episódios, para o mediumClassic 5 mil e o originalClassic 2mil. O número de treinos foi se reduzindo por limitações computacionais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implementação do Q-learning\n",
    "\n",
    "A implementação segue o seguinte algoritmo:\n",
    "\n",
    "* No construtor:\n",
    "    * Inicializa um dicionário para armazenar $Q[S,A]$\n",
    "    * Inicializa os parâmetros utilizados na Q-Function\n",
    "    * Inicializa atributos utilizados para avaliar a recompensa  \n",
    "  \n",
    "  \n",
    "* Função getAction():\n",
    "    * Analisa qual o estado atual\n",
    "    * Avalia a recompensa\n",
    "    * Atualiza Q-Table\n",
    "    * Atualiza os atributos da classe\n",
    "    * Escolhe uma nova ação\n",
    "  \n",
    "\n",
    "* Função final():\n",
    "    * Analisa qual o estado atual\n",
    "    * Avalia a recompensa\n",
    "    * Atualiza Q-Table \n",
    "    * Reseta os atributos da classe\n",
    "    * Finaliza o episódio\n",
    "\n",
    "\n",
    "No arquivo Agents.py possui a implementação da classe QLearnAgent que será apresentada e explicada aqui. Segue abaixo o código completo e a explicação para as principais funções"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Inicialmente temos o construtor da classe incializando a Q-Table com um dicionário e definindo outros atributos que serão utilizados.\n",
    "\n",
    "```python\n",
    "class QLearnAgent(Agent):\n",
    "\n",
    "    # Inicializa o Agent com os atributos necessários\n",
    "    def __init__(self, numTraining = 10):\n",
    "\n",
    "        # Parâmetros do Q-Learning \n",
    "        self.alpha = 0.2\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.8\n",
    "        self.qValues = dict()\n",
    "\n",
    "        # Acompanha resultados durante o episódio\n",
    "        self.numTraining = int(numTraining)\n",
    "        self.episodesSoFar = 0\n",
    "        self.actionsSoFar = 0\n",
    "        self.totalReward = 0\n",
    "        \n",
    "        # Usado para criar estado e recompensas\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.lastScore = None\n",
    "        self.lastNumFood = None\n",
    "        self.lastCaps = None\n",
    "        self.lastDistGhost = None\n",
    "        self.lastDistFood = None\n",
    "        self.doNotEat = None\n",
    "        self.ghostWasNear = False\n",
    "        self.lastFoodPosition = None\n",
    "\n",
    "        # Escreve em arquivos diferente dependendo do número de treino\n",
    "        if(self.getNumTraining() < 2500):\n",
    "            with open('episodesResults3.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "        elif(self.getNumTraining() < 5500):\n",
    "            with open('episodesResults2.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "        else:\n",
    "            with open('episodesResults1.txt','w') as f:\n",
    "                f.write(\"\")\n",
    "```\n",
    "A seguir são algumas funções para acessar, setar e incrementar os atributos.\n",
    "\n",
    "```python\n",
    "\n",
    "    # Funções para acessar os atributos\n",
    "\n",
    "    def incrementEpisodesSoFar(self):\n",
    "        self.episodesSoFar += 1\n",
    "\n",
    "    def getEpisodesSoFar(self):\n",
    "        return self.episodesSoFar\n",
    "    \n",
    "    def incrementAcionsSoFar(self):\n",
    "        self.actionsSoFar += 1\n",
    "    \n",
    "    def getAcionsSoFar(self):\n",
    "        return self.actionsSoFar\n",
    "\n",
    "    def getNumTraining(self):\n",
    "        return self.numTraining\n",
    "\n",
    "    def setEpsilon(self, value):\n",
    "        self.epsilon = value\n",
    "\n",
    "    def getAlpha(self):\n",
    "        return self.alpha\n",
    "\n",
    "    def setAlpha(self, value):\n",
    "        self.alpha = value\n",
    "\n",
    "    def getGamma(self):\n",
    "        return self.gamma\n",
    "```\n",
    "\n",
    "A função seguinte recebe o estado do jogo e os movimentos legais do pacman e retorna uma string descrevendo o estado do conjunto S\n",
    "\n",
    "```python\n",
    "    # Cria o estado baseado em direções legais, distância do fantasma mais próximo e sua posição relativa e da comida mais próxima\n",
    "    def createState(self,state,legal):\n",
    "        posG = np.array(state.getGhostPositions())\n",
    "        posP = np.array(state.getPacmanPosition())\n",
    "        posFood = state.getFood()\n",
    "        numFood = state.getNumFood()\n",
    "\n",
    "        minDistGhost = 99999\n",
    "        indG = None\n",
    "        for i in range(len(posG)):\n",
    "            if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "                indG =  i\n",
    "\n",
    "        if(posG[indG][0] > posP[0]):\n",
    "            if(posG[indG][1] > posP[1]):\n",
    "                ghostRelativePosition = \"upright\"\n",
    "            elif(posG[indG][1] == posP[1]):\n",
    "                ghostRelativePosition = \"right\"\n",
    "            else:\n",
    "                ghostRelativePosition = \"downright\"\n",
    "\n",
    "        elif(posG[indG][0] == posP[0]):\n",
    "            if(posG[indG][1] > posP[1]):\n",
    "                ghostRelativePosition = \"up\"\n",
    "            else:\n",
    "                ghostRelativePosition = \"down\"\n",
    "        else:\n",
    "            if(posG[indG][1] > posP[1]):\n",
    "                ghostRelativePosition = \"upleft\"\n",
    "            elif(posG[indG][1] == posP[1]):\n",
    "                ghostRelativePosition = \"left\"\n",
    "            else:\n",
    "                ghostRelativePosition = \"downleft\"\n",
    "\n",
    "        if(numFood > 0):\n",
    "            minDistFood = 99999\n",
    "            minFood = None\n",
    "            for x in range(posFood.width):\n",
    "                for y in range(posFood.height):\n",
    "                    if(posFood[x][y]):\n",
    "                        food = np.array([x,y])\n",
    "                        distFood = np.linalg.norm(food-posP)\n",
    "                        if(distFood<minDistFood):\n",
    "                            minDistFood = distFood\n",
    "                            minFood = food\n",
    "            \n",
    "            if(minFood[0] > posP[0]):\n",
    "                if(minFood[1] > posP[1]):\n",
    "                    foodRelativePosition = \"upright\"\n",
    "                elif(minFood[1] == posP[1]):\n",
    "                    foodRelativePosition = \"right\"\n",
    "                else:\n",
    "                    foodRelativePosition = \"downright\"\n",
    "\n",
    "            elif(minFood[0] == posP[0]):\n",
    "                if(minFood[1] > posP[1]):\n",
    "                    foodRelativePosition = \"up\"\n",
    "                else:\n",
    "                    foodRelativePosition = \"down\"\n",
    "            else:\n",
    "                if(minFood[1] > posP[1]):\n",
    "                    foodRelativePosition = \"upleft\"\n",
    "                elif(minFood[1] == posP[1]):\n",
    "                    foodRelativePosition = \"left\"\n",
    "                else:\n",
    "                    foodRelativePosition = \"downleft\"\n",
    "            self.lastFoodPosition  = foodRelativePosition\n",
    "        else:\n",
    "            foodRelativePosition = self.lastFoodPosition\n",
    "\n",
    "        if(minDistGhost < 3):\n",
    "            ghostNear = True\n",
    "        else:\n",
    "            ghostNear = False\n",
    "        \n",
    "        # Retorna uma string descrevendo o estado\n",
    "        return str(legal)+str(ghostNear)+str(ghostRelativePosition)+str(foodRelativePosition)\n",
    "```\n",
    "A função seguinte inicializa um novo Q(s,a)\n",
    "\n",
    "```python\n",
    "\n",
    "    # Caso o estado ainda não exista é gerado um para cada ação legal com valor Q = 0\n",
    "    def initializeQValues(self, pacmanState, legal):\n",
    "        self.qValues[pacmanState] = dict()\n",
    "        for action in legal:\n",
    "            if action not in self.qValues[pacmanState]:\n",
    "                self.qValues[pacmanState][action] = 0.0\n",
    "```\n",
    "A seguinte função é utilizada para avaliar a recompensa da última ação e atualizar o valor na Q-Table\n",
    "```python\n",
    "\n",
    "    # Calcula a recompensa da última ação e atualiza  valor Q do estado e sua ação através da função do Q-learning\n",
    "    def updateQValue(self, state,pacmanState, final_step=False):\n",
    "        \n",
    "        posG = np.array(state.getGhostPositions())\n",
    "        posP = np.array(state.getPacmanPosition())\n",
    "        posFood = state.getFood()\n",
    "\n",
    "        numFoodEat = self.lastNumFood - state.getNumFood()\n",
    "        numCapsEat = self.lastCaps - len(state.getCapsules())\n",
    "\n",
    "        minDistGhost = 99999\n",
    "        for i in range(len(posG)):\n",
    "            if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "        \n",
    "        minDistFood = 99999\n",
    "        for x in range(posFood.width):\n",
    "            for y in range(posFood.height):\n",
    "                if(posFood[x][y]):\n",
    "                    food = np.array([x,y])\n",
    "                    distFood = np.linalg.norm(food-posP)\n",
    "                    if(distFood<minDistFood):\n",
    "                        minDistFood = distFood\n",
    "\n",
    "        if(self.ghostWasNear):\n",
    "            if(minDistGhost >= self.lastDistGhost):\n",
    "                didNotEat = 30\n",
    "            else:\n",
    "                didNotEat = -30\n",
    "        else:\n",
    "            if(numFoodEat == 0 or numCapsEat == 0):\n",
    "                if(minDistFood > self.lastDistFood):\n",
    "                    didNotEat = -15\n",
    "                else:\n",
    "                    didNotEat = 15\n",
    "            else:\n",
    "                didNotEat = 0\n",
    "\n",
    "        reward = numFoodEat * 10 + numCapsEat * 20 + didNotEat\n",
    "        self.totalReward += reward\n",
    "\n",
    "        max_Q_value = 0.0\n",
    "        if not final_step:\n",
    "            max_Q_value = max(list(self.qValues[pacmanState].values()))\n",
    "        self.qValues[self.lastState][self.lastAction] += (self.alpha * (reward + self.gamma * max_Q_value - self.qValues[self.lastState][self.lastAction]))\n",
    "```\n",
    "A função seguinte escolhe uma nova ação, com possibilidade de ser uma ação aleatória.\n",
    "\n",
    "```python\n",
    "\n",
    "    # Escolha a nova ação balanceando exploration e exploitation\n",
    "    def epsilonGreedy(self, state,pacmanState, legal):\n",
    "        if Directions.STOP in legal:\n",
    "            legal.remove(Directions.STOP)\n",
    "\n",
    "        if not self.ghostWasNear:\n",
    "            if(self.lastAction!= None):\n",
    "                if (Directions.REVERSE[self.lastAction] in legal) and len(legal)>1:\n",
    "                    legal.remove(Directions.REVERSE[self.lastAction])\n",
    "\n",
    "        probability = random.random()\n",
    "\n",
    "        if probability < self.epsilon: # Escolhe ação aleatória\n",
    "            random_action = random.choice(legal)\n",
    "            return random_action\n",
    "\n",
    "        max_Q_action = None # Escolhe a que já sabe ser melhor\n",
    "        for action in legal:\n",
    "            if max_Q_action == None:\n",
    "                max_Q_action = action\n",
    "            if self.qValues[pacmanState][action] > self.qValues[pacmanState][max_Q_action]:\n",
    "                max_Q_action = action\n",
    "        return max_Q_action\n",
    "```\n",
    "As duas funções a seguir são somente utilizadas para atualizar ou resetar os atributos da classe\n",
    "\n",
    "```python\n",
    "\n",
    "    # Atualiza atributos para um novo estado\n",
    "    def updateAttributes(self, state,pacmanState, legal):\n",
    "\n",
    "        posG = np.array(state.getGhostPositions())\n",
    "        posP = np.array(state.getPacmanPosition())\n",
    "        posFood = state.getFood()\n",
    "\n",
    "        self.lastNumFood = state.getNumFood()\n",
    "        self.lastCaps = len(state.getCapsules())\n",
    "\n",
    "        minDistGhost = 99999\n",
    "        for i in range(len(posG)):\n",
    "            if(np.linalg.norm(posG[i]-posP) < minDistGhost):\n",
    "                minDistGhost = np.linalg.norm(posG[i]-posP)\n",
    "        \n",
    "        minDistFood = 99999\n",
    "        for x in range(posFood.width):\n",
    "            for y in range(posFood.height):\n",
    "                if(posFood[x][y]):\n",
    "                    food = np.array([x,y])\n",
    "                    distFood = np.linalg.norm(food-posP)\n",
    "                    if(distFood<minDistFood):\n",
    "                        minDistFood = distFood\n",
    "        \n",
    "        if(minDistGhost <= 3.5 ):\n",
    "            self.ghostWasNear = True\n",
    "        else:\n",
    "            self.ghostWasNear = False\n",
    "        self.lastDistGhost = minDistGhost\n",
    "        self.lastDistFood = minDistFood\n",
    "        self.lastState = self.createState(state,legal)\n",
    "        self.lastAction = self.epsilonGreedy(state,pacmanState, legal)\n",
    "        self.lastScore = state.getScore()\n",
    "\n",
    "    # Reseta todos atributos (no fim do episódio)\n",
    "    def resetAttributes(self):\n",
    "        self.totalReward = 0\n",
    "        self.actionsSoFar = 0\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.lastScore = None\n",
    "```\n",
    "\n",
    "A função getAction é chamada a cada passo do pacman e realiza os passos do Q-Learning utilizando as funções já implementadas\n",
    "\n",
    "```python\n",
    "\n",
    "    # Escolhe uma ação, inicializa o estado que o pacman está caso não exista ainda e move o pacman\n",
    "    def getAction(self, state):\n",
    "\n",
    "        legal = state.getLegalPacmanActions()\n",
    "\n",
    "        if Directions.STOP in legal: # Remove a ação de parar\n",
    "            legal.remove(Directions.STOP)\n",
    "        \n",
    "        if not self.ghostWasNear: # Se não possuir fantasmas próximos impede o pacman reverter\n",
    "            if(self.lastAction!= None):\n",
    "                if (Directions.REVERSE[self.lastAction] in legal) and len(legal)>1:\n",
    "                    legal.remove(Directions.REVERSE[self.lastAction])\n",
    "        \n",
    "        pacmanState = self.createState(state,legal) # Analisa o estado atual\n",
    "\n",
    "        if pacmanState not in self.qValues: # Insere o estado na Q-Table\n",
    "            self.initializeQValues(pacmanState, legal)\n",
    "\n",
    "        if self.lastState != None: # Avalia a recompensa e atualiza Q-Table\n",
    "            self.updateQValue(state,pacmanState)\n",
    "\n",
    "        self.updateAttributes(state,pacmanState, legal) # Atualiza os atributos da classe\n",
    "\n",
    "        self.incrementAcionsSoFar()\n",
    "\n",
    "        return self.lastAction\n",
    "```\n",
    "A função final é chamada após uma derrota ou vitória encerrando o episódio\n",
    "\n",
    "```python\n",
    "\n",
    "    # Chamada no fim do episódio e atualiza o último estágio\n",
    "    def final(self, state):\n",
    "        legal = state.getLegalPacmanActions()\n",
    "\n",
    "        if Directions.STOP in legal:\n",
    "            legal.remove(Directions.STOP)\n",
    "        \n",
    "        if not self.ghostWasNear:\n",
    "            if(self.lastAction!= None):\n",
    "                if (Directions.REVERSE[self.lastAction] in legal) and len(legal)>1:\n",
    "                    legal.remove(Directions.REVERSE[self.lastAction])\n",
    "\n",
    "        if self.lastState != None:\n",
    "            self.updateQValue(state,self.createState(state,legal), final_step=True)\n",
    "\n",
    "\n",
    "        if(self.getNumTraining() < 2500):\n",
    "            with open('episodesResults3.txt','a') as f:\n",
    "                f.write(str(self.totalReward)+\" \"+str(self.getAcionsSoFar())+\" \"+str(state.getScore())+\"\\n\")\n",
    "        elif(self.getNumTraining() < 5500):\n",
    "            with open('episodesResults2.txt','a') as f:\n",
    "                f.write(str(self.totalReward)+\" \"+str(self.getAcionsSoFar())+\" \"+str(state.getScore())+\"\\n\")\n",
    "        else:\n",
    "            with open('episodesResults1.txt','a') as f:\n",
    "                f.write(str(self.totalReward)+\" \"+str(self.getAcionsSoFar())+\" \"+str(state.getScore())+\"\\n\")\n",
    "\n",
    "        self.resetAttributes()\n",
    "\n",
    "        self.incrementEpisodesSoFar()\n",
    "\n",
    "        if self.getEpisodesSoFar() == self.getNumTraining():\n",
    "            self.setAlpha(0)\n",
    "            self.setEpsilon(0)\n",
    "\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Execução do Q-Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Para executar os treinos foi utilizado o seguinte código num notebook, o tempo de execução foi 1.84 horas.  \n",
    "Assim foi escrito em arquivos separados os resultados de cada episódio e também os resultados das 10 execuções dos melhores agentes.\n",
    "\n",
    "\n",
    "``` python\n",
    "saida1 = !python pacman.py -p QLearnAgent -x 10000 -n 10010 -l smallClassic \n",
    "with open('results1.txt','w') as f:\n",
    "    for i in saida1:\n",
    "        f.write(str(i)+'\\n')\n",
    "\n",
    "saida2 = !python pacman.py -p QLearnAgent -x 5000 -n 5010 -l mediumClassic \n",
    "with open('results2.txt','w') as f:\n",
    "    for i in saida2:\n",
    "        f.write(str(i)+'\\n')\n",
    "\n",
    "saida3 = !python pacman.py -p QLearnAgent -x 2000 -n 2010 -l originalClassic \n",
    "with open('results3.txt','w') as f:\n",
    "    for i in saida3:\n",
    "        f.write(str(i)+'\\n')\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A seguir segue o código excutado para gerar os gráficos\n",
    "\n",
    "``` python\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "def createGraphs(file,title,YLabel,EpNum,ind,fileGenerated):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        xy = [[int(line.split()[0]),int(line.split()[1]),float(line.split()[2])] for line in lines]\n",
    "\n",
    "    epResults = []\n",
    "    for i in range(len(xy[0])):  \n",
    "        aux1 = []\n",
    "        eps = []\n",
    "        for j in range(len(xy)):\n",
    "            eps.append(j+1)\n",
    "            aux1.append(xy[j][i])\n",
    "        epResults.append(aux1)\n",
    "\n",
    "    fig = plt.figure(figsize=(60, 6))\n",
    "    ax = fig.add_subplot()\n",
    "    ax = plt.axes()\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1000))\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(250))\n",
    "    plt.xlim(0, EpNum)\n",
    "    plt.scatter(eps,epResults[ind])\n",
    "    plt.tick_params(axis='x', which='major', labelsize=30)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=30)\n",
    "    plt.title(title,fontsize=30)\n",
    "    plt.xlabel('Episodes',fontsize=30)\n",
    "    plt.ylabel(YLabel,fontsize=30)\n",
    "    plt.savefig(fileGenerated)\n",
    "\n",
    "createGraphs(\"episodesResults1.txt\",\"Reward x Episodes smallClassic\",\"Total Reward\",10000,0,\"TRsmall.png\")\n",
    "createGraphs(\"episodesResults1.txt\",\"Actions x Episodes smallClassic\",\"Total Actions\",10000,1,\"TAsmall.png\")\n",
    "createGraphs(\"episodesResults1.txt\",\"Score x Episodes smallClassic\",\"Total Score\",10000,2,\"TSsmall.png\")\n",
    "\n",
    "createGraphs(\"episodesResults2.txt\",\"Reward x Episodes mediumClassic\",\"Total Reward\",5000,0,\"TRmedium.png\")\n",
    "createGraphs(\"episodesResults2.txt\",\"Actions x Episodes mediumClassic\",\"Total Actions\",5000,1,\"TAmedium.png\")\n",
    "createGraphs(\"episodesResults2.txt\",\"Score x Episodes mediumClassic\",\"Total Score\",5000,2,\"TSmedium.png\")\n",
    "\n",
    "createGraphs(\"episodesResults3.txt\",\"Reward x Episodes originalClassic\",\"Total Reward\",2000,0,\"TRoriginal.png\")\n",
    "createGraphs(\"episodesResults3.txt\",\"Actions x Episodes originalClassic\",\"Total Actions\",2000,1,\"TAoriginal.png\")\n",
    "createGraphs(\"episodesResults3.txt\",\"Score x Episodes originalClassic\",\"Total Score\",2000,2,\"TSoriginal.png\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Resultados do Q-Learning\n",
    "\n",
    "## Layout smallClassic\n",
    "\n",
    "![image](TRsmall.png)\n",
    "![image](TAsmall.png)\n",
    "![image](TSsmall.png)\n",
    "\n",
    "Pacman emerges victorious! Score: 921  \n",
    "Pacman died! Score: -347  \n",
    "Pacman emerges victorious! Score: 1137  \n",
    "Pacman emerges victorious! Score: 893  \n",
    "Pacman died! Score: -214  \n",
    "Pacman emerges victorious! Score: 967  \n",
    "Pacman died! Score: -109  \n",
    "Pacman emerges victorious! Score: 938  \n",
    "Pacman emerges victorious! Score: 931  \n",
    "Pacman emerges victorious! Score: 884  \n",
    "Average Score: 600.1  \n",
    "Scores: 921.0, -347.0, 1137.0, 893.0, -214.0, 967.0, -109.0, 938.0, 931.0, 884.0  \n",
    "Win Rate: 7/10 (0.70)  \n",
    "Record: Win, Loss, Win, Win, Loss, Win, Loss, Win, Win, Win  \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Layout mediumClassic\n",
    "\n",
    "![image](TRmedium.png)\n",
    "![image](TAmedium.png)\n",
    "![image](TSmedium.png)\n",
    "\n",
    "Pacman emerges victorious! Score: 1257  \n",
    "Pacman emerges victorious! Score: 1320  \n",
    "Pacman emerges victorious! Score: 1272  \n",
    "Pacman emerges victorious! Score: 1462  \n",
    "Pacman died! Score: 171  \n",
    "Pacman emerges victorious! Score: 1247  \n",
    "Pacman emerges victorious! Score: 1270  \n",
    "Pacman emerges victorious! Score: 1317  \n",
    "Pacman died! Score: -126  \n",
    "Pacman died! Score: 72  \n",
    "Average Score: 926.2  \n",
    "Scores: 1257.0, 1320.0, 1272.0, 1462.0, 171.0, 1247.0, 1270.0, 1317.0, -126.0, 72.0  \n",
    "Win Rate: 7/10 (0.70)  \n",
    "Record:  Win, Win, Win, Win, Loss, Win, Win, Win, Loss, Loss  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Layout originalClassic\n",
    "\n",
    "![image](TRoriginal.png)\n",
    "![image](TAoriginal.png)\n",
    "![image](TSoriginal.png)\n",
    "\n",
    "Pacman died! Score: 392  \n",
    "Pacman emerges victorious! Score: 2297  \n",
    "Pacman died! Score: 615  \n",
    "Pacman died! Score: 831  \n",
    "Pacman died! Score: 976  \n",
    "Pacman died! Score: 293  \n",
    "Pacman died! Score: 762  \n",
    "Pacman emerges victorious! Score: 2302  \n",
    "Pacman died! Score: 885  \n",
    "Pacman died! Score: 402  \n",
    "Average Score: 975.5  \n",
    "Scores: 392.0, 2297.0, 615.0, 831.0, 976.0, 293.0, 762.0, 2302.0, 885.0, 402.0  \n",
    "Win Rate: 2/10 (0.20)  \n",
    "Record: Loss, Win, Loss, Loss, Loss, Loss, Loss, Win, Loss, Loss  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaff50d265a8a64705d0ed9614cc6a71ec4b37e412313ecbd3946d24e390e436"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}